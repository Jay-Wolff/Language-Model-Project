{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNWTqadzQqP/P1+XIRlQNtB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jay-Wolff/Language-Model-Project/blob/main/Language_Project_Janissa_Abreu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINAL PROJECT"
      ],
      "metadata": {
        "id": "ADAmuNCm2K_Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrisKx3f2JUK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "max_iters = 5001\n",
        "learning_rate = 3e-4\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # utilize GPU computing\n",
        "eval_interval = 100\n",
        "eval_iters = 200"
      ],
      "metadata": {
        "id": "lR-8ppTW2RAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 64 # Affects the size of the bigram model\n",
        "n_head = 4\n",
        "# head_size = 16\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "t1Xk5F22RWBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UAiSAXr5EGI",
        "outputId": "8a2955e8-5e35-49c1-e1ce-918ff3ed7c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-20 23:10:40--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-05-20 23:10:40 (18.8 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "aGsOPf3m5Kdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(len(train_data), len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbCq_Plc5Plw",
        "outputId": "5cc104dc-a8d6-4227-a543-f00db82c7790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1003854 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A self-attention block\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)     # process location information\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)   # process token information\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout) # At this moment, the rate is 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "66y9HKSoOyII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device) # send data to GPU\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "5cTvq1zA5Qz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() # A function modifier to improve efficiency\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "hI4v2Ep15Ut_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "GaoiVnv9O9Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "WI4byEqqPyoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionModel4(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "zVMNTiFLPx8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))      # ResNet structure is used\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "K1g5RjMxPaGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkGqKcVgPtWo",
        "outputId": "fd28d655-721c-49f3-fb74-9cc1491b8c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SelfAttentionModel4()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "import time #time\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Format the start, end, and elapsed times for display\n",
        "formatted_start_time = time.strftime('%X', time.localtime(start_time))\n",
        "formatted_end_time = time.strftime('%X', time.localtime(end_time))\n",
        "\n",
        "print(f\"Start Time: {formatted_start_time}\")\n",
        "print(f\"End Time: {formatted_end_time}\")\n",
        "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-08jUQzP7SN",
        "outputId": "f6fb203e-9307-4b95-b262-4e3eeb876806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.224065 M parameters\n",
            "step 0: train loss 4.3371, val loss 4.3293\n",
            "step 100: train loss 2.9616, val loss 2.9863\n",
            "step 200: train loss 2.6914, val loss 2.6970\n",
            "step 300: train loss 2.5921, val loss 2.5930\n",
            "step 400: train loss 2.5458, val loss 2.5491\n",
            "step 500: train loss 2.5169, val loss 2.5188\n",
            "step 600: train loss 2.4923, val loss 2.4951\n",
            "step 700: train loss 2.4745, val loss 2.4819\n",
            "step 800: train loss 2.4575, val loss 2.4634\n",
            "step 900: train loss 2.4410, val loss 2.4484\n",
            "step 1000: train loss 2.4203, val loss 2.4318\n",
            "step 1100: train loss 2.3964, val loss 2.4070\n",
            "step 1200: train loss 2.3701, val loss 2.3869\n",
            "step 1300: train loss 2.3353, val loss 2.3534\n",
            "step 1400: train loss 2.3064, val loss 2.3256\n",
            "step 1500: train loss 2.2753, val loss 2.2939\n",
            "step 1600: train loss 2.2458, val loss 2.2687\n",
            "step 1700: train loss 2.2226, val loss 2.2478\n",
            "step 1800: train loss 2.2017, val loss 2.2269\n",
            "step 1900: train loss 2.1781, val loss 2.2063\n",
            "step 2000: train loss 2.1588, val loss 2.1923\n",
            "step 2100: train loss 2.1375, val loss 2.1746\n",
            "step 2200: train loss 2.1135, val loss 2.1512\n",
            "step 2300: train loss 2.0902, val loss 2.1385\n",
            "step 2400: train loss 2.0698, val loss 2.1204\n",
            "step 2500: train loss 2.0497, val loss 2.1057\n",
            "step 2600: train loss 2.0268, val loss 2.0882\n",
            "step 2700: train loss 2.0077, val loss 2.0725\n",
            "step 2800: train loss 1.9927, val loss 2.0584\n",
            "step 2900: train loss 1.9720, val loss 2.0412\n",
            "step 3000: train loss 1.9604, val loss 2.0335\n",
            "step 3100: train loss 1.9457, val loss 2.0226\n",
            "step 3200: train loss 1.9284, val loss 2.0118\n",
            "step 3300: train loss 1.9151, val loss 1.9977\n",
            "step 3400: train loss 1.9015, val loss 1.9909\n",
            "step 3500: train loss 1.8921, val loss 1.9826\n",
            "step 3600: train loss 1.8735, val loss 1.9752\n",
            "step 3700: train loss 1.8651, val loss 1.9697\n",
            "step 3800: train loss 1.8544, val loss 1.9623\n",
            "step 3900: train loss 1.8443, val loss 1.9544\n",
            "step 4000: train loss 1.8343, val loss 1.9523\n",
            "step 4100: train loss 1.8247, val loss 1.9386\n",
            "step 4200: train loss 1.8126, val loss 1.9360\n",
            "step 4300: train loss 1.8009, val loss 1.9268\n",
            "step 4400: train loss 1.7972, val loss 1.9253\n",
            "step 4500: train loss 1.7867, val loss 1.9188\n",
            "step 4600: train loss 1.7803, val loss 1.9149\n",
            "step 4700: train loss 1.7704, val loss 1.9098\n",
            "step 4800: train loss 1.7656, val loss 1.9086\n",
            "step 4900: train loss 1.7560, val loss 1.9006\n",
            "step 5000: train loss 1.7495, val loss 1.8953\n",
            "Start Time: 23:10:55\n",
            "End Time: 23:23:26\n",
            "Elapsed Time: 750.59 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy4HB1ju5vaG",
        "outputId": "74323aaa-a740-4b55-8210-c8916cfb27ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Biteslamblel traument lood asceve conin,\n",
            "Are I Gramford Kinglal the deaubis to teles\n",
            "die! with thou.\n",
            "Citioronates I't had carconous and at deam fore\n",
            "sormeofordy sinons if Lordew.\n",
            "\n",
            "BOLULICET:\n",
            "Hemiont my his compery mand his meath hand them that\n",
            "Anchalent them chadedy hould madunes good areass\n",
            "To Enceadin your whild ronceapty uponerlishs,\n",
            "Be yold sleeeve thouse ar my so, the 'eng heave\n",
            "And grads parchour bod eyard my of markly;\n",
            "To his the womere it the yourr hor densles,\n",
            "Thinkesss strequercess for fince, livew,\n",
            "A the son fulendig the beight gratue,\n",
            "Untake I and have drains? for whome.\n",
            "\n",
            "Fincest I madgad the shall to: ponir his as begor'st mat\n",
            "Terry not of sleplects of blad.\n",
            "Pening wellg onecly laiv neatself;\n",
            "Mark it muss, and your beark welks,\n",
            "And must thern noicliate on boinrow dome and\n",
            "roveatagh woen no clegnaterm, it rold the pread,\n",
            "I thun in Done\n",
            "\n",
            "Lecord:\n",
            "by me yourd hony corthand, hast is thy dreath:\n",
            "O! his that but yours you'lk'd hose you my your\n",
            "Whep, deable'st to I rafther have plorrow.\n",
            "\n",
            "CAPETIO:\n",
            "Will fartewelion, shalt, when injanstron of wilt!\n",
            "My in all, my alwore: caush, and your with a with:\n",
            "And wengry strean your corntain,\n",
            "Ber a tin Gomant ruoinst\n",
            "Fer ther I lome not bestiles a faind sleeves:\n",
            "Therep Thour of Man you whicke cauck your a prifes,\n",
            "Thasss not blead this the hund pevearl.\n",
            "\n",
            "ANTOLIO:\n",
            "This those ladore your prest teor'd therem owar\n",
            "Your I meprahtints agaguen on Herven too,\n",
            "Unvanctes beck, and hathou app, good the othe gathx.\n",
            "\n",
            "QUEEET OF MAPABELLA:\n",
            "Edway, gively nother-drokloio,\n",
            "O theree spitting and wasged a devess agendly,\n",
            "Then foer I saven am mast out head wased\n",
            "Onger imptervet grocts, this best to hear coinds,\n",
            "You himandure; batter hoarth, ovic so,\n",
            "She hads not\n",
            "Meare thou than sue of funith colised to your\n",
            "Dood him shale tenceed streefs, Where; jeove we thiqpsties;\n",
            "I andwergence!\n",
            "Herstanced you mates orr thy hath tin of onvengs:\n",
            "If to day stay his eded mEd, my his cheest with wilt:\n",
            "To Richile muchapin haver but fear, and me a ad\n",
            "Alm in wilded you my tparcers oneater\n",
            "The not your and desse and my virne:\n",
            "But conn crevisveries! wAnce to-speaseds, not\n",
            "Had im the lon almpice to I so; chomise.\n",
            "O thee thenle it beye, home borroth.\n",
            "\n",
            "AUFFIOL:\n",
            "I dis lord? word, I my younk, sile I and rrostlenger:\n",
            "No is owich coaw groves boche baight not.\n",
            "To Henat he tindliok, and of saith'led.\n",
            "\n",
            "DUKE OF MARI:\n",
            "A gay there: you butth the not it gut a rattizer\n",
            "His his deargest be of hoven his duke.\n",
            "\n",
            "WARWICHAMI:\n",
            "Fatelathsus with the hist rum thery sumy.\n",
            "\n",
            "PALIO:\n",
            "I was woulther Hostrathr, and rehies my andss:\n",
            "You If afff how gie? man bestrok to heir\n",
            "yey branceat? you composts cass the is he teld\n",
            "To peartime.\n",
            "\n",
            "Shot madere you? entan you sor?\n",
            "\n",
            "\n",
            "COMY:\n",
            "Hoverner:\n",
            "Wereathes I do wordser would yoursto.\n",
            "\n",
            "KING Lirlan:\n",
            "The shen seich dooke.\n",
            "\n",
            "LEONIUM:\n",
            "Co-diray, you nought rever throm him.\n",
            "\n",
            "KING YORK:\n",
            "I belead word to hath dother our of greats;\n",
            "What belean hen enot thindiness so: hap a sun\n",
            "sigh eith tide, that poor trisi.\n",
            "\n",
            "Thy:\n",
            "Kere Pasitizes you gracce, mo, I dieth: thre a they joy;\n",
            "What in heave a Rincourast.\n",
            "\n",
            "DUplandiond Gowntle of trights buranigh.\n",
            "\n",
            "COMANCET:\n",
            "Mas prat, coussting thercef.\n",
            "Forlow, and thank for cold to myst as bebes\n",
            "My the dauefurage? Now, a bring of speace,\n",
            "And dead the feall oter quaiebe, of shad a lot\n",
            "Come, her I fould then comfort ontre;\n",
            "and cofeny nowseial:\n",
            "And and to grest so though and romase,--\n",
            "our your hear me; breart faintor, mostaid thee,\n",
            "To to will near out good have my ared.\n",
            "To pon, learrces us purst at our oveen in sould;\n",
            "Foor which ton boideson! cure the a wis borrne,\n",
            "In shall onever one of togly vitherme,\n",
            "So worn tilemoss of mast pood a ther.\n",
            "Notsh, now for tot werm?\n",
            "\n",
            "POLIO:\n",
            "No foold--leas me manders,--a gock.\n",
            "\n",
            "BALLINIUS:\n",
            "Whath vengort brofins!\n",
            "\n",
            "WARWABELLLIUS:\n",
            "I'l That yather's begcres mety a nog'en ding.\n",
            "But warrres moven to buse? I'lll strennour\n",
            "Is how binder fore lace owll, or thy---bake.\n",
            "\n",
            "MORGARELE:\n",
            "Your a whash atty thhan be our that.\n",
            "Nee Eddwen, oneke ap, my at annd as thim:\n",
            "art streeps speace, spow a the this are ogak a me\n",
            "To, teatted thoner a stoone; the, aren he'er how a\n",
            "Brook in of covose: than breing my onear.\n",
            "\n",
            "PRIUCES:\n",
            "Was him, the as thats, said that trow is as:\n",
            "Which oweest weither, connobless, and in it' toung\n",
            "And fistal.\n",
            "\n",
            "HENRD:\n",
            "In diver for meadlied, is to not a prouse,\n",
            "No will am your know-make of sun.\n",
            "\n",
            "Firse it himse butuny.\n",
            "I ollses like your the goom hour I broth;\n",
            "Uplab off stin make the his of too to,\n",
            "The severs\n",
            "Wapitor, is your uponest man.\n",
            "\n",
            "CLARIO:\n",
            "Nay? must I aporst a fallow?\n",
            "\n",
            "Nayan:\n",
            "Ere it yourng hathan: Seee, put moard Lorss a\n",
            "And inless.\n",
            "Yether, showle to friven that thealy art,\n",
            "Brovend and shall fesceed, wromble you your growst bees:\n",
            "War, bean; and so? not gailloust's and my leather.\n",
            "Shalling gress my was a of 'tisgaiess how,\n",
            "Jomethers, to nigh a would whoses Lunck of of\n",
            "That not bewayd the immplosests sodenger press:\n",
            "Hose mang ownts sealch,-busee luse good:\n",
            "Whome is incetes bumusee grasice this gradaie.\n",
            "\n",
            "\n",
            "ANNCUMA:\n",
            "Relives crioush onker her saive allied to repaselk,\n",
            "And\n"
          ]
        }
      ]
    }
  ]
}